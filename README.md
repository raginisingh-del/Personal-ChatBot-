Personal-ChatBot â€” Chatbot using Ollama + Streamlit

Personal-ChatBot is a lightweight, privacy-friendly local chatbot built with Streamlit and powered by Ollama models like deepseek-r1:1.5b. It supports multiple chat sessions, live response streaming, and even shows the modelâ€™s hidden thought process (using <think> tags, if supported by the model).

ğŸš€ Features
âœ… Multiple Chat Sessions â€“ Create, switch, and resume chats easily from the sidebar (requires code implementation beyond the base template).
âœ… Live Streaming Output â€“ Watch the bot respond in real time.
âœ… Hidden Thought Display â€“ View the modelâ€™s internal reasoning in an expandable â€œğŸ§  Thoughtsâ€ section (if model supports <think> tags).
âœ… Local Privacy â€“ Works completely offline using Ollama â€” no external API calls.
âœ… Clean Math Rendering â€“ Converts LaTeX-style symbols (like \frac, \times, etc.) into readable Unicode (requires additional libraries/logic).
âœ… Session Persistence â€“ Each chat session is uniquely saved with timestamps.

ğŸ§  How It Works
Section
Description
Frontend
Streamlit provides a simple and elegant web interface.
Ollama runs a local model (e.g., deepseek-r1:1.5b) via the REST API endpoint http://localhost:11434/api/generate.
Streaming Responses
The chatbot listens to a streaming response from Ollama and renders each chunk live.
Session Management
Chats are stored in st.session_state for persistence, including both visible messages and hidden thoughts.

âš™ï¸ Installation
1ï¸âƒ£ Install Ollama
Download and install Ollama from the official website.
2ï¸âƒ£ Pull a model
Pull the recommended model or the model currently configured in your app.py:
ollama pull deepseek-r1:1.5b

You can replace this with any compatible model (like llama3, mistral, etc.).
3ï¸âƒ£ Install dependencies
Navigate to your project folder, activate your virtual environment (.\venv\Scripts\Activate.ps1), and install:
pip install streamlit requests

ğŸ§© Run the App
Save the code as app.py, then start the Streamlit server:
streamlit run app.py

Make sure Ollama is running in the background (it starts automatically when you run a model).
Then open the app in your browser at: ğŸ‘‰ http://localhost:8501

ğŸ’¡ Usage
Type your message in the chat input at the bottom.
The chatbot will stream its answer live.
Click on ğŸ§  Modelâ€™s Thought Process to see what the model â€œthoughtâ€ before answering.
Start a new chat or revisit old ones using the sidebar.

ğŸ§‘â€ğŸ’» Author
ğŸ‘¤ [Ragini Singh] | Built with  using Python, Streamlit, and Ollama.

ğŸ“œ License
This project is open-source under the MIT License â€” feel free to modify and improve it.ed by [Ragini Singh]
